{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 23s 11ms/step - loss: 0.3648 - accuracy: 0.8965 - val_loss: 0.2210 - val_accuracy: 0.9364\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1742 - accuracy: 0.9507 - val_loss: 0.1357 - val_accuracy: 0.9592\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.1233 - accuracy: 0.9645 - val_loss: 0.1089 - val_accuracy: 0.9671\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17920c58e50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4]\n",
      "[7 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "# TESTING #1\n",
    "# Save the model to disk.\n",
    "model.save_weights('cnn.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Varying network depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 28s 14ms/step - loss: 0.2758 - accuracy: 0.9185 - val_loss: 0.1239 - val_accuracy: 0.9631\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.1105 - accuracy: 0.9667 - val_loss: 0.0829 - val_accuracy: 0.9732\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.0840 - accuracy: 0.9742 - val_loss: 0.0748 - val_accuracy: 0.9764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1792312b9d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 3 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn2.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding Dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 12s 5ms/step - loss: 0.4760 - accuracy: 0.8551 - val_loss: 0.2651 - val_accuracy: 0.9245\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3127 - accuracy: 0.9070 - val_loss: 0.2098 - val_accuracy: 0.9402\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2623 - accuracy: 0.9220 - val_loss: 0.1664 - val_accuracy: 0.9534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc014c2640>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Dropout(0.5), #Dropout layer\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 3 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn4_1.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding Fully-connected Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.2710 - accuracy: 0.9196 - val_loss: 0.1324 - val_accuracy: 0.9605\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.1026 - accuracy: 0.9690 - val_loss: 0.0790 - val_accuracy: 0.9769\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.0683 - accuracy: 0.9792 - val_loss: 0.0689 - val_accuracy: 0.9782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x179256168b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'), #Fully connected layer\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn4.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Messing with convolution parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.4573 - accuracy: 0.8768 - val_loss: 0.2127 - val_accuracy: 0.9374\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1933 - accuracy: 0.9424 - val_loss: 0.1611 - val_accuracy: 0.9517\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1574 - accuracy: 0.9525 - val_loss: 0.1436 - val_accuracy: 0.9587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17925fba9d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,    padding='same',    activation='relu',  ), #Modified and added parameters\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 3 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn2.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.2645 - accuracy: 0.9210 - val_loss: 0.0984 - val_accuracy: 0.9697\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0831 - accuracy: 0.9746 - val_loss: 0.0662 - val_accuracy: 0.9794\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0579 - accuracy: 0.9819 - val_loss: 0.0590 - val_accuracy: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17923115d30>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,    padding='same',    activation='relu',  ), #Modified and added parameters\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'), #Fully connected layer\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 8 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn5.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images.\n",
    "##train_images = (train_images / 255) - 0.5\n",
    "##test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "##train_images = np.expand_dims(train_images, axis=3)\n",
    "##test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4213 - accuracy: 0.8642 - val_loss: 0.1175 - val_accuracy: 0.9634\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1794 - accuracy: 0.9418 - val_loss: 0.0745 - val_accuracy: 0.9752\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1388 - accuracy: 0.9558 - val_loss: 0.0559 - val_accuracy: 0.9818\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc0390ec70>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,    padding='same',    activation='relu',  ), #Modified and added parameters\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Dropout(0.5), #Dropout layer\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'), #Fully connected layer\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 5 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn6.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 3\n",
    "Adding one more filter (num_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images.\n",
    "#train_images = (train_images / 255) - 0.5\n",
    "#test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "##train_images = np.expand_dims(train_images, axis=3)\n",
    "##test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "num_filters = 9\n",
    "filter_size = 3\n",
    "pool_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 9s 4ms/step - loss: 0.3521 - accuracy: 0.8887 - val_loss: 0.0887 - val_accuracy: 0.9734\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.1430 - accuracy: 0.9546 - val_loss: 0.0559 - val_accuracy: 0.9806\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1146 - accuracy: 0.9630 - val_loss: 0.0470 - val_accuracy: 0.9850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc1e693880>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,    padding='same',    activation='relu',  ), #Modified and added parameters\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Dropout(0.5), #Dropout layer\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'), #Fully connected layer\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn7.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 4\n",
    "Adding 3 more filter (num_filters=12)\n",
    "\n",
    "filter_size and pool size not modified because we think those sizes are perfect since their objective is to detect image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images.\n",
    "#train_images = (train_images / 255) - 0.5\n",
    "#test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "##train_images = np.expand_dims(train_images, axis=3)\n",
    "##test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "num_filters = 12\n",
    "filter_size = 3\n",
    "pool_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.3523 - accuracy: 0.8880 - val_loss: 0.0934 - val_accuracy: 0.9701\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1335 - accuracy: 0.9575 - val_loss: 0.0626 - val_accuracy: 0.9791\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.1007 - accuracy: 0.9683 - val_loss: 0.0437 - val_accuracy: 0.9859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc00210940>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,    padding='same',    activation='relu',  ), #Modified and added parameters\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Dropout(0.5), #Dropout layer\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'), #Fully connected layer\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 5 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn8.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 5\n",
    "filter_size and pool size not modified because we think those sizes are perfect since their objective is to detect image features.\n",
    "\n",
    "Incremented epochs to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images.\n",
    "#train_images = (train_images / 255) - 0.5\n",
    "#test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "##train_images = np.expand_dims(train_images, axis=3)\n",
    "##test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "num_filters = 12\n",
    "filter_size = 3\n",
    "pool_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0698 - accuracy: 0.8793 - val_loss: 0.0169 - val_accuracy: 0.9745\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0257 - accuracy: 0.9622 - val_loss: 0.0120 - val_accuracy: 0.9837\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 18s 9ms/step - loss: 0.0202 - accuracy: 0.9703 - val_loss: 0.0090 - val_accuracy: 0.9856\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0176 - accuracy: 0.9743 - val_loss: 0.0087 - val_accuracy: 0.9869\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0161 - accuracy: 0.9762 - val_loss: 0.0078 - val_accuracy: 0.9881\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0144 - accuracy: 0.9791 - val_loss: 0.0069 - val_accuracy: 0.9900\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0141 - accuracy: 0.9797 - val_loss: 0.0067 - val_accuracy: 0.9900\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0133 - accuracy: 0.9806 - val_loss: 0.0069 - val_accuracy: 0.9890\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0130 - accuracy: 0.9808 - val_loss: 0.0067 - val_accuracy: 0.9907\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0124 - accuracy: 0.9822 - val_loss: 0.0067 - val_accuracy: 0.9898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc19ad38b0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,    padding='same',    activation='relu',  ), #Modified and added parameters\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Dropout(0.5), #Dropout layer\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'), #Fully connected layer\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=10,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 5 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn9.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images.\n",
    "#train_images = (train_images / 255) - 0.5\n",
    "#test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "##train_images = np.expand_dims(train_images, axis=3)\n",
    "##test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "num_filters = 12\n",
    "filter_size = 3\n",
    "pool_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.3112 - accuracy: 0.9032 - val_loss: 0.0691 - val_accuracy: 0.9781\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.1254 - accuracy: 0.9608 - val_loss: 0.0539 - val_accuracy: 0.9830\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.0963 - accuracy: 0.9704 - val_loss: 0.0405 - val_accuracy: 0.9866\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0811 - accuracy: 0.9736 - val_loss: 0.0334 - val_accuracy: 0.9890\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0736 - accuracy: 0.9766 - val_loss: 0.0341 - val_accuracy: 0.9888\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0667 - accuracy: 0.9788 - val_loss: 0.0309 - val_accuracy: 0.9898\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0617 - accuracy: 0.9800 - val_loss: 0.0269 - val_accuracy: 0.9923\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0593 - accuracy: 0.9805 - val_loss: 0.0298 - val_accuracy: 0.9903\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0576 - accuracy: 0.9815 - val_loss: 0.0335 - val_accuracy: 0.9876\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0526 - accuracy: 0.9835 - val_loss: 0.0302 - val_accuracy: 0.9907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cc041be940>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,    padding='same',    activation='relu',  ), #Modified and added parameters\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Dropout(0.5), #Dropout layer\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'), #Fully connected layer\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=10,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn10.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Convolutional Neural Network\n",
    "\n",
    "Basandonos en el código de https://victorzhou.com/blog/keras-cnn-tutorial/ los primeros resultados obtenidos para el \"val_accuracy\" fueron entre 95% y 97%\n",
    "\n",
    " -- Resultados obtenidos ejecutando el notebook en dos computadoras.\n",
    "\n",
    "Con los \"Recommended Experiments\" del tutorial, fuimos variando distintos valores, parámetros, capas, por separado en distintos experimentos:\n",
    "\n",
    "- Varying network depth: Se agrego una capa de convolución 2D obteniendo un \"val_accuracy\" maximo de 0.9727 y 0.9764 en cada computadora respectivamente.\n",
    "  \n",
    "- Adding Dropout layers: Obtuvimos un \"val_accuracy\" maximo de 0.9534 y 0.9573 en cada computadora respectivamente.\n",
    "\n",
    "- Adding Fully-connected Layers: Con la función de activación \"Relu\", obtuvimos un \"val_accuracy\" maximo de 0.9782 y 0.9774 en cada computadora respectivamente.\n",
    "\n",
    "- Messing with convolution parameters: Añadimos varios parametros a la capa de convolución 2D, obteniendo un \"val_accuracy\" maximo de 0.9587 en ambas computadoras.\n",
    "\n",
    "Haciendo experimentos propios de la misma manera variando valores, parámetros, capas, etc, y juntando varios de estos parametros en un mismo modelo, tenemos los siguientes resultados:\n",
    "\n",
    "- Experimento 1: Sin dropout obtuvimos un \"val_accuracy\" maximo de : 0.9800 y 0.9830\n",
    "- Experimento 2: Con dropout y manteniendo los demas parametros, obtuvimos un \"val_accuracy\" maximo de : 0.9818 y 0.9821\n",
    "- Experimento 3: Añadiendo un filtro (num_filters = 9) y manteniendo los anteriores parametros obtuvimos un \"val_accuracy\" maximo de : 0.9850 y 0.9822\n",
    "- Experimento 4: Añadiendo 3 filtros más (num_filters = 12), filter_size y pool size no se modifican porque creemos que esos tamaños son perfectos ya que su objetivo es detectar características de la imagen. Obtuvimos un \"val_accuracy\" maximo de: 0.9859 y 0.9851\n",
    "\n",
    "  -- Observacion: hasta este punto los mejores valores de \"val_accuracy\" fueron obtenidos en el 3er epoch\n",
    "\n",
    "- Experimento 5: Cambiando el loss_function a 'binary_crossentropy' y el número de epochs a 10 y manteniendo los anteriores parametros, obtuvimos: 0.9907 (Epoch 9) y 0.9908 (Epoch 10)\n",
    "- Experimento 6: Cambiando el loss_function a 'categorical_crossentropy' y manteniendo los anteriores parametros, obtuvimos 0.9923 (Epoch 7) y 0.9917 (Epoch 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
