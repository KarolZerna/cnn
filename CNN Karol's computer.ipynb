{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.3721 - accuracy: 0.8937 - val_loss: 0.2163 - val_accuracy: 0.9361\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 18s 10ms/step - loss: 0.1888 - accuracy: 0.9451 - val_loss: 0.1475 - val_accuracy: 0.9572\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 20s 10ms/step - loss: 0.1374 - accuracy: 0.9611 - val_loss: 0.1215 - val_accuracy: 0.9626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbb70cf9670>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the images.\n",
    "train_images = (train_images / 255) - 0.5\n",
    "test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4]\n",
      "[7 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "# TESTING #1\n",
    "# Save the model to disk.\n",
    "model.save_weights('cnn.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:5])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1)) # [7, 2, 1, 0, 4]\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:5]) # [7, 2, 1, 0, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Varying network depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 38s 20ms/step - loss: 0.2644 - accuracy: 0.9216 - val_loss: 0.1153 - val_accuracy: 0.9650\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 33s 17ms/step - loss: 0.1094 - accuracy: 0.9679 - val_loss: 0.0885 - val_accuracy: 0.9727\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 30s 16ms/step - loss: 0.0834 - accuracy: 0.9751 - val_loss: 0.0878 - val_accuracy: 0.9714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbb531b7400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 5 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn2.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding Dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 31s 16ms/step - loss: 0.4176 - accuracy: 0.8745 - val_loss: 0.2073 - val_accuracy: 0.9402\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.2561 - accuracy: 0.9231 - val_loss: 0.1598 - val_accuracy: 0.9541\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.2263 - accuracy: 0.9313 - val_loss: 0.1398 - val_accuracy: 0.9573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbb52b75460>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Dropout(0.5), #Dropout layer\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 3 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn4_1.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding Fully-connected Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3063 - accuracy: 0.9107 - val_loss: 0.1557 - val_accuracy: 0.9532\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.1133 - accuracy: 0.9668 - val_loss: 0.0865 - val_accuracy: 0.9747\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0757 - accuracy: 0.9773 - val_loss: 0.0714 - val_accuracy: 0.9774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbb4178c670>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(num_filters, filter_size, input_shape=(28, 28, 1)),\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'), #Fully connected layer\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn4.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Messing with convolution parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.4573 - accuracy: 0.8768 - val_loss: 0.2127 - val_accuracy: 0.9374\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1933 - accuracy: 0.9424 - val_loss: 0.1611 - val_accuracy: 0.9517\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1574 - accuracy: 0.9525 - val_loss: 0.1436 - val_accuracy: 0.9587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17925fba9d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,    padding='same',    activation='relu',  ), #Modified and added parameters\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 6 9 0 6 9 0 1 5 9 7 3 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn2.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personal experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.2564 - accuracy: 0.9219 - val_loss: 0.0948 - val_accuracy: 0.9685\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0775 - accuracy: 0.9758 - val_loss: 0.0600 - val_accuracy: 0.9799\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.0558 - accuracy: 0.9830 - val_loss: 0.0507 - val_accuracy: 0.9830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbb32122ca0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,    padding='same',    activation='relu',  ), #Modified and added parameters\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "    MaxPooling2D(pool_size=pool_size),\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'), #Fully connected layer\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 8 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn5.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images.\n",
    "##train_images = (train_images / 255) - 0.5\n",
    "##test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "##train_images = np.expand_dims(train_images, axis=3)\n",
    "##test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.3988 - accuracy: 0.8747 - val_loss: 0.1094 - val_accuracy: 0.9650\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.1740 - accuracy: 0.9455 - val_loss: 0.0741 - val_accuracy: 0.9762\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 20s 11ms/step - loss: 0.1381 - accuracy: 0.9573 - val_loss: 0.0589 - val_accuracy: 0.9821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbb0e3ef310>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,    padding='same',    activation='relu',  ), #Modified and added parameters\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Dropout(0.5), #Dropout layer\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'), #Fully connected layer\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 5 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn6.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 3\n",
    "Adding one more filter (num_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images.\n",
    "#train_images = (train_images / 255) - 0.5\n",
    "#test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "##train_images = np.expand_dims(train_images, axis=3)\n",
    "##test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "num_filters = 9\n",
    "filter_size = 3\n",
    "pool_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.3726 - accuracy: 0.8812 - val_loss: 0.0984 - val_accuracy: 0.9697\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.1668 - accuracy: 0.9469 - val_loss: 0.0681 - val_accuracy: 0.9771\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1308 - accuracy: 0.9595 - val_loss: 0.0530 - val_accuracy: 0.9822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbb1cf66850>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,    padding='same',    activation='relu',  ), #Modified and added parameters\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Dropout(0.5), #Dropout layer\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'), #Fully connected layer\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 5 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn7.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 4\n",
    "Adding 3 more filter (num_filters=12)\n",
    "\n",
    "filter_size and pool size not modified because we think those sizes are perfect since their objective is to detect image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images.\n",
    "#train_images = (train_images / 255) - 0.5\n",
    "#test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "##train_images = np.expand_dims(train_images, axis=3)\n",
    "##test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "num_filters = 12\n",
    "filter_size = 3\n",
    "pool_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 21s 11ms/step - loss: 0.3467 - accuracy: 0.8903 - val_loss: 0.0769 - val_accuracy: 0.9754\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.1315 - accuracy: 0.9594 - val_loss: 0.0566 - val_accuracy: 0.9809\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 19s 10ms/step - loss: 0.1032 - accuracy: 0.9678 - val_loss: 0.0435 - val_accuracy: 0.9851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbb0e0836a0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,    padding='same',    activation='relu',  ), #Modified and added parameters\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Dropout(0.5), #Dropout layer\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'), #Fully connected layer\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=3,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn8.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 5\n",
    "filter_size and pool size not modified because we think those sizes are perfect since their objective is to detect image features.\n",
    "\n",
    "Incremented epochs to 10.\n",
    "\n",
    "We also added another con2D layer.\n",
    "\n",
    "We also changed loss function to 'binary crossentropy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images.\n",
    "#train_images = (train_images / 255) - 0.5\n",
    "#test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "##train_images = np.expand_dims(train_images, axis=3)\n",
    "##test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "num_filters = 12\n",
    "filter_size = 3\n",
    "pool_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0525 - accuracy: 0.9002 - val_loss: 0.0156 - val_accuracy: 0.9721\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 25s 13ms/step - loss: 0.0245 - accuracy: 0.9574 - val_loss: 0.0110 - val_accuracy: 0.9805\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 33s 18ms/step - loss: 0.0197 - accuracy: 0.9655 - val_loss: 0.0099 - val_accuracy: 0.9823\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 37s 20ms/step - loss: 0.0168 - accuracy: 0.9705 - val_loss: 0.0079 - val_accuracy: 0.9866\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0153 - accuracy: 0.9734 - val_loss: 0.0074 - val_accuracy: 0.9876\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 31s 17ms/step - loss: 0.0138 - accuracy: 0.9764 - val_loss: 0.0069 - val_accuracy: 0.9888\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 29s 16ms/step - loss: 0.0130 - accuracy: 0.9773 - val_loss: 0.0070 - val_accuracy: 0.9877\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 22s 12ms/step - loss: 0.0121 - accuracy: 0.9786 - val_loss: 0.0059 - val_accuracy: 0.9900\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.0112 - accuracy: 0.9806 - val_loss: 0.0056 - val_accuracy: 0.9908\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 44s 24ms/step - loss: 0.0109 - accuracy: 0.9814 - val_loss: 0.0057 - val_accuracy: 0.9903\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbb1231ea30>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,    padding='same',    activation='relu',  ), #Modified and added parameters\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Dropout(0.5), #Dropout layer\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'), #Fully connected layer\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=10,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 8 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn9.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.losses.MeanSquaredError at 0x7fbb13038760>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimento 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as experiment 5, but we changed the loss function back to 'categorical crossentropy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images.\n",
    "#train_images = (train_images / 255) - 0.5\n",
    "#test_images = (test_images / 255) - 0.5\n",
    "\n",
    "# Reshape the images.\n",
    "##train_images = np.expand_dims(train_images, axis=3)\n",
    "##test_images = np.expand_dims(test_images, axis=3)\n",
    "\n",
    "num_filters = 12\n",
    "filter_size = 3\n",
    "pool_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.3107 - accuracy: 0.9018 - val_loss: 0.0756 - val_accuracy: 0.9770\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.1343 - accuracy: 0.9578 - val_loss: 0.0526 - val_accuracy: 0.9836\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 23s 13ms/step - loss: 0.1037 - accuracy: 0.9672 - val_loss: 0.0424 - val_accuracy: 0.9875\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 24s 13ms/step - loss: 0.0847 - accuracy: 0.9727 - val_loss: 0.0396 - val_accuracy: 0.9875\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.0745 - accuracy: 0.9767 - val_loss: 0.0350 - val_accuracy: 0.9888\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 32s 17ms/step - loss: 0.0701 - accuracy: 0.9778 - val_loss: 0.0299 - val_accuracy: 0.9906\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.0645 - accuracy: 0.9797 - val_loss: 0.0312 - val_accuracy: 0.9896\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.0616 - accuracy: 0.9803 - val_loss: 0.0281 - val_accuracy: 0.9904\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 23s 12ms/step - loss: 0.0584 - accuracy: 0.9817 - val_loss: 0.0310 - val_accuracy: 0.9905\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0566 - accuracy: 0.9819 - val_loss: 0.0275 - val_accuracy: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbb1302f820>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model.\n",
    "model = Sequential([\n",
    "  Conv2D(\n",
    "    num_filters,\n",
    "    filter_size,\n",
    "    input_shape=(28, 28, 1),\n",
    "    strides=2,    padding='same',    activation='relu',  ), #Modified and added parameters\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "  Conv2D(num_filters, filter_size), #Layer added\n",
    "  MaxPooling2D(pool_size=pool_size),\n",
    "  Dropout(0.5), #Dropout layer\n",
    "  Flatten(),\n",
    "  Dense(64, activation='relu'), #Fully connected layer\n",
    "  Dense(10, activation='softmax'),\n",
    "])\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(\n",
    "  'adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "# Train the model.\n",
    "model.fit(\n",
    "  train_images,\n",
    "  to_categorical(train_labels),\n",
    "  epochs=10,\n",
    "  validation_data=(test_images, to_categorical(test_labels)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk.\n",
    "model.save_weights('cnn10.h5')\n",
    "\n",
    "# Load the model from disk later using:\n",
    "# model.load_weights('cnn.h5')\n",
    "\n",
    "# Predict on the first 5 test images.\n",
    "predictions = model.predict(test_images[:20])\n",
    "\n",
    "# Print our model's predictions.\n",
    "print(np.argmax(predictions, axis=1))\n",
    "\n",
    "# Check our predictions against the ground truths.\n",
    "print(test_labels[:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
